{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_to_encode = False # if you need to clean and encode texts\n",
    "need_sample = False # if you need to sample some items of datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import encoder\n",
    "\n",
    "if (need_to_encode):\n",
    "    model = encoder.parse_model('models/ruwikiruscorpora_upos_skipgram_300_2_2018.vec')\n",
    "    \n",
    "    print(len(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if (need_to_encode):\n",
    "    train_texts = encoder.load_from_pickle('data/train_texts_cleaned_short.pickle')\n",
    "    test_texts = encoder.load_from_pickle('data/test_texts_cleaned_short.pickle')\n",
    "    \n",
    "    print(f'{len(train_texts)} train texsts are loaded')\n",
    "    print(f'{len(test_texts)} test texsts are loaded')\n",
    "\n",
    "    cleaned_train_texts = list(map(lambda t: encoder.clean_text(t, model), train_texts))\n",
    "    cleaned_test_texts = list(map(lambda t: encoder.clean_text(t, model), test_texts))\n",
    "    \n",
    "    print(f'{len(cleaned_train_texts)} train texts are cleaned')\n",
    "    print(f'{len(cleaned_test_texts)} test texts are cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode texts to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (need_to_encode):\n",
    "    train_encoded = list(map(lambda t: encoder.encode_text(t, model, (256, 300)), cleaned_train_texts))\n",
    "    test_encoded = list(map(lambda t: encoder.encode_text(t, model, (256, 300)), cleaned_test_texts))\n",
    "\n",
    "    print(f'{len(train_encoded)} train texts are encoded')\n",
    "    print(f'{len(test_encoded)} test texts are encoded')\n",
    "    \n",
    "    print(train_encoded[1])\n",
    "\n",
    "    encoder.save_to_pickle(train_encoded, 'data/train_texts_cleaned_short_encoded.pickle')\n",
    "    encoder.save_to_pickle(test_encoded, 'data/test_texts_cleaned_short_encoded.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4492 train_labels are loaded\n",
      "4492 train_encoded texts are loaded\n",
      "1000 test labels are loaded\n",
      "1000 test encoded texts are loaded\n"
     ]
    }
   ],
   "source": [
    "train_labels = encoder.load_from_pickle('data/train_labels2.pickle')\n",
    "train_encoded = encoder.load_from_pickle('data/train_texts_cleaned_short_encoded.pickle')\n",
    "print(f'{len(train_labels)} train_labels are loaded')\n",
    "print(f'{len(train_encoded)} train_encoded texts are loaded')\n",
    "\n",
    "test_labels = encoder.load_from_pickle('data/val_labels2.pickle')\n",
    "test_encoded = encoder.load_from_pickle('data/test_texts_cleaned_short_encoded.pickle')\n",
    "print(f'{len(test_labels)} test labels are loaded')\n",
    "print(f'{len(test_encoded)} test encoded texts are loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "if (need_sample):\n",
    "    train_number = 260\n",
    "    test_number = 30\n",
    "\n",
    "    train_sample = []\n",
    "    train_labels_sample = []\n",
    "    test_sample = []\n",
    "    test_labels_sample = []\n",
    "\n",
    "    for i in range(0, train_number):\n",
    "        j = random.randint(0, 4491)\n",
    "        train_sample.append(train_encoded[j])\n",
    "        train_labels_sample.append(train_labels[j])\n",
    "\n",
    "    for i in range(0, test_number):\n",
    "        j = random.randint(0, 999)\n",
    "        test_sample.append(test_encoded[j])\n",
    "        test_labels_sample.append(test_labels[j])\n",
    "\n",
    "    train_encoded = train_sample\n",
    "    train_labels = train_labels_sample\n",
    "    test_encoded = test_sample\n",
    "    test_labels = test_labels_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4492 train encoded texts are converted to tensors\n",
      "1000 test encoded texts are converted to tensors\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_tensors_x = torch.FloatTensor(train_encoded).view(4492, 1, 256, 300)\n",
    "train_tensors_y = torch.FloatTensor(train_labels)\n",
    "test_tensors_x = torch.FloatTensor(test_encoded).view(1000, 1, 256, 300)\n",
    "test_tensors_y = torch.FloatTensor(test_labels)\n",
    "\n",
    "train_dataset = TensorDataset(train_tensors_x, train_tensors_y)\n",
    "test_dataset = TensorDataset(test_tensors_x, test_tensors_y)\n",
    "\n",
    "print(f'{len(train_dataset)} train encoded texts are converted to tensors')\n",
    "print(f'{len(test_dataset)} test encoded texts are converted to tensors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data to train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "#I took train and val sets in a ratio of 7 to 3\n",
    "train, val = random_split(train_dataset, [3992, 500])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=4)\n",
    "val_loader = DataLoader(val, batch_size=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "Iteration 0, loss = 1.0493\n",
      "Iteration 100, loss = 0.9622\n",
      "Iteration 200, loss = 0.9518\n",
      "Iteration 300, loss = 0.5827\n",
      "Iteration 400, loss = 0.6969\n",
      "Iteration 500, loss = 0.4569\n",
      "Iteration 600, loss = 0.6760\n",
      "Iteration 700, loss = 0.5846\n",
      "Iteration 800, loss = 0.6443\n",
      "Iteration 900, loss = 0.4479\n",
      "Iteration 0, loss = 0.5385\n",
      "Iteration 100, loss = 0.9487\n",
      "Iteration 200, loss = 0.9006\n",
      "Iteration 300, loss = 0.5692\n",
      "Iteration 400, loss = 0.6944\n",
      "Iteration 500, loss = 0.3447\n",
      "Iteration 600, loss = 0.4529\n",
      "Iteration 700, loss = 0.3216\n",
      "Iteration 800, loss = 0.5805\n",
      "Iteration 900, loss = 0.6234\n",
      "Iteration 0, loss = 0.5460\n",
      "Iteration 100, loss = 1.0900\n",
      "Iteration 200, loss = 0.7038\n",
      "Iteration 300, loss = 0.2966\n",
      "Iteration 400, loss = 0.6052\n",
      "Iteration 500, loss = 0.3135\n",
      "Iteration 600, loss = 0.2557\n",
      "Iteration 700, loss = 0.1816\n",
      "Iteration 800, loss = 0.4332\n",
      "Iteration 900, loss = 0.6031\n"
     ]
    }
   ],
   "source": [
    "from models.convnet import ConvNet\n",
    "from model_utils import train\n",
    "import torch.optim\n",
    "\n",
    "#[conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "im_size = (256, 300, 1)\n",
    "conv_params = [(256, 7, 2), (512, 5, 2)]\n",
    "linear_params = [32, 2]\n",
    "learning_rate = 5e-2\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('using device:', device)\n",
    "    \n",
    "model = ConvNet(im_size, conv_params, linear_params)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "train(model, train_loader, val_loader, optimizer, device, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import eval_model\n",
    "\n",
    "pred, groundtruth = eval_model(test_loader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.821\n",
      "f1 score: 0.802\n",
      "recall: 0.896\n",
      "precision: 0.726\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "print(f'accuracy: {accuracy_score(pred, groundtruth):.3f}')\n",
    "print(f'f1 score: {f1_score(pred, groundtruth):.3f}')\n",
    "print(f'recall: {recall_score(pred, groundtruth):.3f}')\n",
    "print(f'precision: {precision_score(pred, groundtruth):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
